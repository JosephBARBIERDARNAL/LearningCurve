{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a13ffa-56fb-43c0-8d53-918bad2f60a5",
   "metadata": {},
   "source": [
    "# Decision tree and Random forest\n",
    "\n",
    "#### Main concepts\n",
    "\n",
    "**Entropy**\\\n",
    "It's a measure of disorder (or purity, we'll see what it means). If we are working with a dichotomous variablee, we can interpet it as follow: if $E=0$ (resp. $E=1$), all observations are in the first (resp. second) class. Otherwise, if $E=0.5$, the disorder is at the highest (the categories are equally divided). More formally, we can describe it as the following.\n",
    "$$E = \\sum_{i=1}^c -p_i log_2 p_i,~\\in [0,1]$$\n",
    "$$c = the~number~of~classes$$\n",
    "Have in mind that $E \\in [0,1]$ is only true when working with dichotomous variable, even tough the logic explained before is the same for variables with more than 2 categories. Let's try an example: first we create 3 different variables with different number of modalities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68aecad-74a3-4c01-98cd-377691ac3b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first variable (dichotomous): ['Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes']\n",
      "My second variable: ['Maybe', 'Maybe', 'No', 'No', 'Yes', 'Maybe', 'Yes', 'No', 'Yes']\n",
      "My thirds variable: ['B', 'A', 'D', 'A', 'D', 'C', 'B', 'B', 'A', 'D', 'C', 'C']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "two_modalities = ['Yes'] * 7 + ['No'] * 3\n",
    "random.shuffle(two_modalities)\n",
    "print(f\"My first variable (dichotomous): {two_modalities}\")\n",
    "\n",
    "three_modalities = ['Yes'] * 3 + ['No'] * 3 + ['Maybe'] * 3\n",
    "random.shuffle(three_modalities)\n",
    "print(f\"My second variable: {three_modalities}\")\n",
    "\n",
    "four_modalities = ['A'] * 3 + ['B'] * 3 + ['C'] * 3 + ['D'] * 3\n",
    "random.shuffle(four_modalities)\n",
    "print(f\"My thirds variable: {four_modalities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e26c843f-b0ee-4d43-9a38-7aebb90164d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the variable (computed by hand, with 2 modalities): 0.8812908992306927\n",
      "Entropy of the variable (computed with scipy, with 2 modalities): 0.8812908992306927\n",
      "\n",
      "Entropy of the variable (computed by hand, with 3 modalities): 1.584962500721156\n",
      "Entropy of the variable (computed with scipy, with 3 modalities): 1.584962500721156\n",
      "\n",
      "Entropy of the variable (computed by hand, with 4 modalities): 2.0\n",
      "Entropy of the variable (computed with scipy, with 4 modalities): 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modules\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "#define a function\n",
    "def my_entropy(data):\n",
    "    classes = set(data)\n",
    "    n = len(data)\n",
    "    ent = 0\n",
    "    pk = []\n",
    "    for c in classes:\n",
    "        p = data.count(c) / n\n",
    "        pk.append(p)\n",
    "        ent -= p * math.log(p,2)\n",
    "    print(f\"Entropy of the variable (computed by hand, with {len(classes)} modalities): {ent}\")\n",
    "    \n",
    "    ent2 = entropy(np.array(pk),base=2)\n",
    "    print(f\"Entropy of the variable (computed with scipy, with {len(classes)} modalities): {ent2}\\n\")\n",
    "\n",
    "my_entropy(two_modalities)\n",
    "my_entropy(three_modalities)\n",
    "my_entropy(four_modalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289f993-449c-4872-a2b1-e07caf4dfff7",
   "metadata": {},
   "source": [
    "There are other similar measure as entropy, like [Gini index](https://blog.quantinsti.com/gini-index/#:~:text=Gini%20Index%20is%20a%20powerful,of%20a%20decision%20tree%20model.). A Gini index of 0 means that the sample is perfectly homgeneous (every observation is from the same class) and an index of 1 means is the opposite. We describe it formally we the following formula:\n",
    "$$G_{ini} = 1 - \\sum_{i=1}^c p_i^2,~\\in [0,1]$$\n",
    "$$c = the~number~of~classes$$\n",
    "\n",
    "\n",
    "**But what are decision trees?** \\\n",
    "Decision trees are a way to formally define a decision process, based on some condition (i.e. what we are looking for in ML). More concretly, you can see it that way (image taken from [Kaggle](https://www.kaggle.com/learn)):\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"imageDT.png\" alt=\"Decision tree example\" width=\"600\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Our goal here, for a given target variable (ex: the house price) and a dataset, find what are the nodes (ex: \"Does house have more than 2 Bedrooms\") that predicts the best the target variable. We will train a model on a training dataset, in order to find those nodes, and then test the model on new data in order to compare what the model predict and the true value (we will see some example after).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Algorithms used in decision tree\n",
    "\n",
    "Since decision trees can be used either for regression task (predict a continuous variable, like monthly income) or classification task (predict a categorical variable, like marital status), we will have to see both of them. We will only talk about the [CART](https://fr.wikipedia.org/wiki/Algorithme_CART) (classification and regression tree) algorithm, even tough there are lots of other with different specificities. The CART algorithm uses the Gini index as a main metric to evaluate each split for classification task and least square for feature selection for regression task.\n",
    "\n",
    "\n",
    "**CART algorithm**\\\n",
    "We will implement our own CART algorithm for a classification task. First, we create a Gini index function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f38a0267-4238-4e9c-aca3-41e476cb143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini index (with 2 modalities): 0.42000000000000004\n",
      "Gini index (with 3 modalities): 0.6666666666666667\n",
      "Gini index (with 4 modalities): 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_gini_index(data):\n",
    "    \n",
    "    sum_of_p = 0 #init the index at 0 since it's a sum\n",
    "    classes = set(data) #define a list of all different classes\n",
    "    n = len(data) #save the length of data\n",
    "    \n",
    "    #iterate over all classes and add the square proportion of each modality to the gini index\n",
    "    for c in classes:\n",
    "        p = data.count(c) / n\n",
    "        sum_of_p += p**2\n",
    "        \n",
    "    #define gini using the sum\n",
    "    gini = 1 - sum_of_p\n",
    "    \n",
    "    #print the index value\n",
    "    print(f\"Gini index (with {len(classes)} modalities): {gini}\")\n",
    "    \n",
    "    #output\n",
    "    return gini\n",
    "\n",
    "my_gini_index(two_modalities)\n",
    "my_gini_index(three_modalities)\n",
    "my_gini_index(four_modalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ef2a6-a384-406f-a8e9-7411b5b1844d",
   "metadata": {},
   "source": [
    "We have consistent results: the higher the number of modalities, the higher the Gini index. Before going further, we need to define a dataset that will be used for the continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32a29e44-2afb-42e9-82d0-19c42654206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Education     Income Gender Happiness\n",
      "0       high        low  women        No\n",
      "1     medium       none  other       Yes\n",
      "2       high        low    men        No\n",
      "3       high       high  women       Yes\n",
      "4     medium        low  women       Yes\n",
      "5        low     medium  other       Yes\n",
      "6        low  very high    men        No\n",
      "7       high     medium    men        No\n",
      "8       high        low  other        No\n",
      "9        low       none    men       Yes\n",
      "10      high     medium  women       Yes\n",
      "11       low        low    men        No\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#define a sample size\n",
    "size = 12\n",
    "\n",
    "#create 3 variables\n",
    "education = np.random.choice(['low', 'medium', 'high'],\n",
    "                             size=size,\n",
    "                             replace=True)\n",
    "income = np.random.choice(['none', 'low', 'medium', 'high', 'very high'],\n",
    "                                 size=size,\n",
    "                                 replace=True)\n",
    "gender = np.random.choice(['men', 'women', 'other'],\n",
    "                                 size=size,\n",
    "                                 replace=True)\n",
    "happiness = np.random.choice(['Yes', 'No'],\n",
    "                                 size=size,\n",
    "                                 replace=True)\n",
    "\n",
    "#create a pandas dataframe\n",
    "data = {'Education': education, 'Income': income, 'Gender': gender, 'Happiness': happiness}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#display the df\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bc737-a767-4c9d-bf33-5ac86434ba79",
   "metadata": {},
   "source": [
    "We want to create a decision tree that discriminates if a person is happy or not, based on the person education level, income level and gender. Important: my data are randomly generated and purely fake (the results obtained are not replicable and cannot be used make inference for real world people). Now let's briefly explain how the algorithm works:\n",
    "- (1) for each of the $k$ (= 3) explanatory variable, we create a contingency table with the target variable (Happiness). Then we compute the Gini index of each modality (low, medium and high, for Education). And we compute the Gini index (sum of  the latter) of each variable. \n",
    "- (2) We select the variable with the least Gini index (the root node) since it means the highest purity and homogeneity.\n",
    "- (3) With the latter variable, we create $c$ branchs (the number of modality of the variable in question). For Education, we have a branch low, medium and high. We are now looking for the split or node to add at the end of each of these branch.\n",
    "- (4) We repeat the step (1) and (2) on each new node but by doing subset of the initial training set (Education == \"low\") until the Gini index equals 0. Once it's the case, it's homogenous and we add a leaf with the only one modality from the contingency table (there can only be one since the Gini index equals 0, by definition). \n",
    "\n",
    "Let's do an example in order to make it clearer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19971d28-8401-4743-b693-7bb60c4c14cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_gini_index() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_modalities):\n\u001b[1;32m     11\u001b[0m     mod \u001b[38;5;241m=\u001b[39m contingency_tab\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mmy_gini_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: my_gini_index() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create a contingency table between two variables\n",
    "contingency_tab = pd.crosstab(df.Education, df.Happiness)\n",
    "\n",
    "#calculate the number of modalities in the explanatory variable\n",
    "number_of_modalities = len(contingency_tab)\n",
    "\n",
    "#compute the Gini index for each modality\n",
    "for i in range(number_of_modalities):\n",
    "    mod = contingency_tab.iloc[i]\n",
    "    print(my_gini_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7f3f7-f3b8-44a8-bcd8-6d95c180fd69",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "# Implementation with Python\n",
    "\n",
    "#### Import of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e55ce-8c74-451d-838c-4ea6f7bc43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "data = pd.read_csv(f\"/Users/josephbarbier/Desktop/appML/models/Iris.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d98ad1-74a7-48cb-b6f7-870473882a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
